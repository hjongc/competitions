{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNs6NdTdJ8eP"
      },
      "source": [
        "# best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQ4fzHA5J8eV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from glob2 import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandasql import sqldf\n",
        "import random\n",
        "import os\n",
        "from numba import jit,njit\n",
        "\n",
        "# 경고 끄기\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "# 시드고정\n",
        "tf.random.set_seed(19970119)\n",
        "random.seed(19970119)\n",
        "np.random.seed(19970119)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6KeVclxJ8ex"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op_HKyOgJ8ex"
      },
      "source": [
        "## 입력 shape 및 형태 정의 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnlo6CdoJ8ey"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def make_Tensor(array):\n",
        "    return tf.convert_to_tensor(array, dtype=tf.float32)\n",
        "\n",
        "@jit\n",
        "def astype_data(data):\n",
        "    df = data.astype(np.float32)\n",
        "    return make_Tensor(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW5tSx9CJ8ez"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23d5CbY2J8e0"
      },
      "source": [
        "## Transformer 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnsaPEZxJ8e0"
      },
      "source": [
        "- encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN-y2wg2J8e1"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHPatYotJ8e2"
      },
      "source": [
        "- build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f53uwkrVJ8e3"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(28)(x) # 4주 예측\n",
        "    return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf-P1DEqJ8e5"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt8olYmCJ8e5"
      },
      "source": [
        "## keras eraly stop, chekpoint 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI2gT91mJ8e6"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def call_back_set(name, epoch, batch_size):\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
        "\n",
        "    if os.path.exists(f'./check') == False:\n",
        "        os.mkdir(f'./check')\n",
        "\n",
        "    filename = f'./check/{name}-{epoch}-{batch_size}.h5'\n",
        "\n",
        "    checkpoint = ModelCheckpoint(filename,\n",
        "                                 monitor='val_loss',\n",
        "                                 verbose=0,\n",
        "                                 save_best_only=True,\n",
        "                                 save_weights_only=True,\n",
        "                                 mode='auto'\n",
        "                                 )\n",
        "    return [early_stopping, checkpoint]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx-muC5oJ8e7"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh0K9WvSJ8e8"
      },
      "source": [
        "## Model 훈련 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5O8y4bqJ8e9"
      },
      "outputs": [],
      "source": [
        "def train(x_train, y_train, x_val, y_val, name, epoch, batch_size, learning_rate = 0.001, verbose = 0):\n",
        "    model = build_model(\n",
        "    x_train.shape[1:],\n",
        "    head_size=256,\n",
        "    num_heads=3,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=3,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.2,\n",
        "    dropout=0.2,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"mean_squared_error\",\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    with tf.device('/device:GPU:1'):\n",
        "        history1 = model.fit(\n",
        "            x_train, y_train,\n",
        "            epochs = epoch,\n",
        "            steps_per_epoch=len(x_train) / batch_size,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(x_val, y_val),\n",
        "            validation_steps=len(x_val) / batch_size,\n",
        "            shuffle=False,\n",
        "            callbacks=call_back_set(name, epoch, batch_size),\n",
        "            verbose=verbose)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLmwSKHnJ8e_"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0sTg94GJ8fA"
      },
      "source": [
        "## 시점 윈도우 생성 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7o0auoqJ8fB"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def time_window(df, t, t_sep):\n",
        "    seq_len = t\n",
        "    seqence_length = seq_len + t_sep\n",
        "\n",
        "    result = []\n",
        "    for index in range(len(df) - seqence_length):\n",
        "        result.append(df[index: index + seqence_length].values)\n",
        "\n",
        "    return np.array(result)\n",
        "\n",
        "@jit\n",
        "def y_time_window(df, t, t_sep):\n",
        "    seq_len = t\n",
        "    seqence_length = seq_len + t_sep\n",
        "\n",
        "    result = []\n",
        "    for index in range(14, len(df) - seqence_length):\n",
        "        result.append(df[index: index + seqence_length].values - df.iloc[index-1])\n",
        "\n",
        "    return np.array(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6x20G1VJ8fF"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqK2s1oZJ8fG"
      },
      "source": [
        "## 데이터 불러오기 및 parameter 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkV5__kvJ8fI"
      },
      "outputs": [],
      "source": [
        "data_list = glob('/content/drive/MyDrive/nongnet_data/data/train/*.csv')\n",
        "epoch = 1000\n",
        "batch = 64\n",
        "tr_del_list = ['단가(원)', '거래량', '거래대금(원)', '경매건수', '도매시장코드', '도매법인코드', '산지코드 '] # train 에서 사용하지 않는 열\n",
        "ts_del_list = ['단가(원)', '거래량', '거래대금(원)', '경매건수', '도매시장코드', '도매법인코드', '산지코드 ', '해당일자_전체평균가격(원)'] # test 에서 사용하지 않는 열\n",
        "check_col = ['일자구분_중순', '일자구분_초순', '일자구분_하순','월구분_10월', '월구분_11월', '월구분_12월', '월구분_1월', '월구분_2월', '월구분_3월', \n",
        "             '월구분_4월','월구분_5월', '월구분_6월', '월구분_7월', '월구분_8월', '월구분_9월'] # 열 개수 맞추기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0BzViNOk9FR"
      },
      "outputs": [],
      "source": [
        "pd.read_csv(data_list[0]).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm8FLNrlhp3y"
      },
      "outputs": [],
      "source": [
        "\n",
        "def feature_engineer(df):\n",
        "       #시간 코사인 인코딩\n",
        "       if df['datadate'].all() == 0:\n",
        "         df['datadate'] = 20180520\n",
        "       df['datetime'] = pd.to_datetime(df['datadate'], format=\"%Y%m%d\")\n",
        "       df['cos_time'] = np.cos(2*np.pi*df['datetime'].dt.dayofyear/365)\n",
        "\n",
        "       # 결측치\n",
        "       df = df.replace(' ', np.nan)\n",
        "       df = df.fillna(0)\n",
        "\n",
        "       # 주산지 초기온도, 최대온도, 최저온도, 평균온도, 강수량, 습도\n",
        "       df['주산지_초기온도'] = (df['주산지_0_초기온도(℃)'] + df['주산지_1_초기온도(℃)'] + df['주산지_2_초기온도(℃)'])/3\n",
        "       df['주산지_최대온도'] = (df['주산지_0_최대온도(℃)'] + df['주산지_1_최대온도(℃)'] + df['주산지_2_최대온도(℃)'])/3\n",
        "       df['주산지_최저온도'] = (df['주산지_0_최저온도(℃)'] + df['주산지_1_최저온도(℃)'] + df['주산지_2_최저온도(℃)'])/3\n",
        "       df['주산지_평균온도'] = (df['주산지_0_평균온도(℃)'] + df['주산지_1_평균온도(℃)'] + df['주산지_2_평균온도(℃)'])/3\n",
        "       df['주산지_강수량'] = (df['주산지_0_강수량(ml)'] + df['주산지_1_강수량(ml)'] + df['주산지_2_강수량(ml)'])/3\n",
        "\n",
        "       # 필요한 컬럼 선택\n",
        "       \n",
        "       market_cols = [] # '단가(원)', '경매건수', '거래량', '거래대금(원)'\n",
        "\n",
        "       market_code_cols = [] # '도매시장코드', '도매법인코드', '산지코드 '\n",
        "\n",
        "       market_statistics_cols = ['해당일자_전체평균가격(원)', \n",
        "                                 '해당일자_전체거래물량(kg)', '일자별_소매가격_최대(원)', '일자별_소매가격_평균(원)', '일자별_소매가격_최소(원)'] \n",
        "       # , '하위가격 거래물량(kg)', '상위가격 거래물량(kg)', '일자별_도매가격_최대(원)', '하위가격 평균가(원)',\n",
        "       #  '상위가격 평균가(원)', \n",
        "       #  '일자별_도매가격_평균(원)', '일자별_도매가격_최소(원)', , \n",
        "       \n",
        "       export_cols = [] # '수출중량(kg)', '수출금액(달러)', '수입중량(kg)', '수입금액(달러)', '무역수지(달러)'\n",
        "       \n",
        "       date_cols = ['datadate', 'cos_time'] # '일자구분_중순', '일자구분_초순', '일자구분_하순', '월구분_10월', '월구분_11월', '월구분_12월', '월구분_1월', '월구분_2월','월구분_3월', '월구분_4월', '월구분_5월', '월구분_6월', '월구분_7월', '월구분_8월', '월구분_9월', 'datetime'\n",
        "\n",
        "       weather_cols = [] \n",
        "      #  '주산지_0_초기온도(℃)', '주산지_1_초기온도(℃)', '주산지_2_초기온도(℃)',\n",
        "      #  '주산지_0_최대온도(℃)', '주산지_1_최대온도(℃)', '주산지_2_최대온도(℃)',\n",
        "      #  '주산지_0_최저온도(℃)', '주산지_1_최저온도(℃)', '주산지_2_최저온도(℃)',\n",
        "      #  '주산지_0_평균온도(℃)', '주산지_1_평균온도(℃)', '주산지_2_평균온도(℃)',\n",
        "      #  '주산지_0_강수량(ml)', '주산지_1_강수량(ml)', '주산지_2_강수량(ml)',\n",
        "      #  '주산지_0_습도(%)', '주산지_1_습도(%)', '주산지_2_습도(%)'\n",
        "\n",
        "       selected_columns = market_cols + market_code_cols + market_statistics_cols + export_cols +  date_cols + weather_cols # train 에서 사용하지 않는 열\n",
        "\n",
        "       df = df[[c for c in df.columns if c in selected_columns]]\n",
        "\n",
        "       return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ-dAEuNJ8fJ"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMvZHAseOARe"
      },
      "outputs": [],
      "source": [
        "# data_list\n",
        "\n",
        "\n",
        "feature_engineer(pd.read_csv(data_list[8]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF2VmW68J8fK"
      },
      "source": [
        "## Train 과정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WgmRJNAuJ8fL"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "\n",
        "losses = []\n",
        "scalers = {}\n",
        "\n",
        "for i in tqdm(data_list):\n",
        "    print(i)\n",
        "    df_number = i.split(\"_\")[-1].split(\".\")[0]\n",
        "    df = pd.read_csv(i)\n",
        "\n",
        "    # 사용할 열 선택 및 index 설정\n",
        "    df = feature_engineer(df)\n",
        "    df.set_index('datadate', drop=True, inplace=True)\n",
        "    \n",
        "    # nan 처리\n",
        "    df = df.replace({' ': np.nan})\n",
        "    df = df.fillna(0)\n",
        "    \n",
        "    # 변수와 타겟 분리\n",
        "    x, y = df[[i for i in df.columns if i != '해당일자_전체평균가격(원)']], df['해당일자_전체평균가격(원)']\n",
        "    scaler = MinMaxScaler()\n",
        "    x = pd.DataFrame(scaler.fit_transform(x))\n",
        "    scalers[df_number] = scaler\n",
        "\n",
        "    # 2주 입력을 통한 이후 4주 예측을 위해 y의 첫 14일을 제외\n",
        "    # y = y[14:]\n",
        "\n",
        "    # time series window 생성\n",
        "    data_x = time_window(x, 13, 1)\n",
        "    data_y = y_time_window(y, 27, 1)\n",
        "\n",
        "    # y의 길이와 같은 길이로 설정\n",
        "    xdata = data_x[:len(data_y)]\n",
        "    ydata = data_y\n",
        "    \n",
        "    # train, validation 분리 (8 : 2)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(xdata, ydata, test_size=0.2, shuffle=False, random_state=119)\n",
        "\n",
        "    # transformer 모델 훈련\n",
        "    transformer = train(astype_data(x_train), y_train, astype_data(x_val), y_val, f'transformer-{df_number}', epoch,\n",
        "                        batch, learning_rate = 0.001, verbose = 0)\n",
        "    \n",
        "    transformer.load_weights(f'./check/transformer-{df_number}-{epoch}-{batch}.h5')\n",
        "    loss = transformer.evaluate(astype_data(x_val), y_val, verbose=0)\n",
        "    print('\\nvalid_loss: ', loss)\n",
        "    losses.append(loss)\n",
        "\n",
        "    if os.path.exists(f'./model') == False:\n",
        "        os.mkdir(f'./model')\n",
        "\n",
        "    # 모델 저장\n",
        "    transformer.save(f'./model/transformer-{df_number}-{epoch}-{batch}.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGBSWYe-Nno-"
      },
      "outputs": [],
      "source": [
        "losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgp2nVPxJ8fR"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNikiLNgJ8fT"
      },
      "source": [
        "## Test 과정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiWO0A8xJ8fU"
      },
      "outputs": [],
      "source": [
        "zero_csv = [0 for i in range(14)]  # 시점이 비어있는 데이터 0으로 채우기 위한 변수\n",
        "\n",
        "for i in tqdm(range(10)):\n",
        "    data_list = glob(f'/content/drive/MyDrive/nongnet_data/data/test/set_{i}/*.csv')\n",
        "    for idx,j in enumerate(data_list):\n",
        "        df = pd.read_csv(j)\n",
        "        \n",
        "        if len(df) == 0:\n",
        "            df['zero_non'] = zero_csv\n",
        "            df = df.fillna(0)\n",
        "            df.drop('zero_non', axis=1, inplace=True)\n",
        "\n",
        "        file_number = j.split('test_')[1].split('.')[0]\n",
        "\n",
        "        # 사용할 열 선택 및 index 설정\n",
        "\n",
        "        df = feature_engineer(df)\n",
        "        df = df[[c for c in df.columns if c != '해당일자_전체평균가격(원)']]\n",
        "\n",
        "        df.set_index('datadate', drop=True, inplace=True)\n",
        "        \n",
        "        # # nan 처리\n",
        "        # df = df.replace({' ': np.nan})\n",
        "        # df = df.fillna(0)\n",
        "\n",
        "        # # train input 과 형상 맞추기\n",
        "        # add_col = [i for i in check_col if i not in df.columns]\n",
        "\n",
        "        # for a in add_col:\n",
        "        #     df[a] = 0\n",
        "\n",
        "        # # ' ' -> nan 으로 변경\n",
        "        # for a in df.columns:\n",
        "        #     df[a] = df[a].replace({' ': np.nan})\n",
        "\n",
        "        # # nan 처리\n",
        "        # df = df.fillna(0)\n",
        "        # # df = df.interpolate(method = 'values')\n",
        "\n",
        "        df = pd.DataFrame(scalers[file_number].transform(df))\n",
        "\n",
        "        # x_test  생성\n",
        "        df_test = astype_data(df.values.reshape(1, df.values.shape[0], df.values.shape[1]))\n",
        "\n",
        "        # model test\n",
        "        if os.path.exists('./model_output') == False:\n",
        "            os.mkdir('./model_output')\n",
        "\n",
        "        if os.path.exists(f'./model_output/set_{i}') == False:\n",
        "            os.mkdir(f'./model_output/set_{i}')\n",
        "\n",
        "        # 해당하는 모델 불러오기\n",
        "        model_test = tf.keras.models.load_model(f'./model/transformer-{file_number}-{epoch}-{batch}.h5')\n",
        "        pred = model_test.predict(df_test)\n",
        "\n",
        "\n",
        "        # 결과 저장\n",
        "        save_df = pd.DataFrame(pred).T\n",
        "        save_df.to_csv(f'./model_output/set_{i}/predict_{file_number}.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PveHHhgHRKI_"
      },
      "outputs": [],
      "source": [
        "# # 사용할 열 선택 및 index 설정\n",
        "#     df = feature_engineer(df)\n",
        "#     df.set_index('datadate', drop=True, inplace=True)\n",
        "    \n",
        "#     # nan 처리\n",
        "#     df = df.replace({' ': np.nan})\n",
        "#     df = df.fillna(0)\n",
        "    \n",
        "#     # 변수와 타겟 분리\n",
        "#     x, y = df[[i for i in df.columns if i != '해당일자_전체평균가격(원)']], df['해당일자_전체평균가격(원)']\n",
        "#     scaler = MinMaxScaler()\n",
        "#     x = pd.DataFrame(scaler.fit_transform(x))\n",
        "#     scalers[df_number] = scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4MQRAWbJ8fW"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXoDxJGKJ8fX"
      },
      "source": [
        "## 정답 제출 파일생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXtt4y4vJ8fY"
      },
      "outputs": [],
      "source": [
        "for k in tqdm(range(10)):\n",
        "  globals()[f'set_df_{k}'] = pd.DataFrame()\n",
        "  answer_df_list = glob(f'./model_output/set_{k}/*.csv') # 예측한 결과 불러오기\n",
        "  pum_list = glob(f'/content/drive/MyDrive/nongnet_data/aT_test_raw/sep_{k}/*.csv') # 기존 test input 불러오기\n",
        "  pummok = [a for a in pum_list if 'pummok' in a.split('/')[-1]]\n",
        "\n",
        "  for i in answer_df_list:\n",
        "    df = pd.read_csv(i)\n",
        "    number = i.split('_')[-1].split('.')[0]\n",
        "\n",
        "    base_number = 0\n",
        "    for p in pummok:\n",
        "      if number == p.split('_')[-1].split('.')[0]:\n",
        "        pum_df = pd.read_csv(p)\n",
        "\n",
        "        if len(pum_df) != 0:\n",
        "           base_number = pum_df.iloc[len(pum_df)-1]['해당일자_전체평균가격(원)']  # 기존 각 sep 마다 test input의 마지막 target 값 가져오기 (변동률 계산을 위해)\n",
        "        else:\n",
        "          base_number = np.nan\n",
        "\n",
        "    globals()[f'set_df_{k}'][f'품목{number}']  = [base_number] + list(df[df.columns[-1]].values) # 각 품목당 순서를 t, t+1 ... t+28 로 변경\n",
        "\n",
        "  globals()[f'set_df_{k}'] = globals()[f'set_df_{k}'][[f'품목{col}' for col in range(37)]] # 열 순서를 품목0 ~ 품목36 으로 변경"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2NbT2FCJ8fZ"
      },
      "source": [
        "- 변동률 계산을 위한 t, t+1 ... t+28 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqT2NTY7J8fa"
      },
      "outputs": [],
      "source": [
        "set_df_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6GWHYtTJ8fb"
      },
      "source": [
        "- 변동률 계산 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7guVyL7J8fc"
      },
      "outputs": [],
      "source": [
        "date = [f'd+{i}' for i in range(1,15)] + ['d+22 ~ 28 평균']\n",
        "\n",
        "for k in range(10):\n",
        "  globals()[f'answer_df_{k}'] = pd.DataFrame()\n",
        "  for c in globals()[f'set_df_{k}'].columns:\n",
        "    base_d = globals()[f'set_df_{k}'][c][0] # 변동률 기준 t 값\n",
        "    ans_1_14 = []\n",
        "    for i in range(14):\n",
        "      ans_1_14.append((globals()[f'set_df_{k}'][c].iloc[i+1])/base_d)\n",
        "    \n",
        "    # ans_22_28 = (globals()[f'set_df_{k}'][c][22:29].mean() - base_d)/base_d # t+22 ~ t+28은 np.mean(t+22 ~ t+28) - t / t\n",
        "    ans_22_28 = (globals()[f'set_df_{k}'][c][22:29].mean())/base_d\n",
        "    globals()[f'answer_df_{k}'][f'{c} 변동률'] = ans_1_14 + [ans_22_28]\n",
        "  \n",
        "  globals()[f'answer_df_{k}']['Set'] = k # set 번호 설정\n",
        "  globals()[f'answer_df_{k}']['일자'] = date # 일자 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kl8cEL6J8fe"
      },
      "source": [
        "- sep 0  ~ sep 9 까지 합치기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8nepRwGJ8ff"
      },
      "outputs": [],
      "source": [
        "# 위에서 계산된 변동률 들을 합쳐주는 과정\n",
        "\n",
        "all_df =pd.DataFrame()\n",
        "for i in range(10):\n",
        "  if i== 0 :\n",
        "    all_df = pd.concat([all_df, globals()[f'answer_df_{i}']],axis=1)\n",
        "  else:\n",
        "    all_df = pd.concat([all_df, globals()[f'answer_df_{i}']])\n",
        "\n",
        "\n",
        "all_df = all_df[['Set','일자'] + list(all_df.columns[:-2])]\n",
        "all_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_Qxfg3_J8fg"
      },
      "outputs": [],
      "source": [
        "answer_df_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WObZCCMwJ8fi"
      },
      "source": [
        "- 정답 양식으로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF_5UI2wJ8fj"
      },
      "outputs": [],
      "source": [
        "# set, 일자 기억하기위해 따로 저장\n",
        "\n",
        "re_set = list(all_df['Set'])\n",
        "re_date = list(all_df['일자'])\n",
        "\n",
        "\n",
        "# 정답 양식 불러오기\n",
        "out_ans = pd.read_csv('/content/drive/MyDrive/nongnet_data/answer_example.csv')\n",
        "\n",
        "# 두 dataframe 합치기 (nan + 숫자 = nan 이용)\n",
        "submit_df = all_df + out_ans\n",
        "\n",
        "submit_df['Set'] = re_set\n",
        "submit_df['일자'] = re_date\n",
        "\n",
        "\n",
        "# 최종 저장\n",
        "submit_df.to_csv('./submit.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBIveJrBJ8fn"
      },
      "source": [
        "- 계산된 변동률 결과물"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8fySZ0aJ8fp"
      },
      "outputs": [],
      "source": [
        "all_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OCf5arQJ8fq"
      },
      "source": [
        "- 제출 양식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gI2ehAsJ8fq"
      },
      "outputs": [],
      "source": [
        "out_ans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwVuAQnjJ8fs"
      },
      "source": [
        "- 제출 양식 반영한 최종 결과물 (**실 제출용**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01gJL4kXJ8fs"
      },
      "outputs": [],
      "source": [
        "submit_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnsReasDlgEb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "interpreter": {
      "hash": "0ac99b679e96e98709c35eb1edc554e4edf87e6f7c8255750be86327ee221383"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit ('nnenv': conda)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
