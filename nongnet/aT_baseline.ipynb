{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErGBb-K_dg0t"
      },
      "source": [
        "# aT 농산품 예측 base line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN-XaBftdlts"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsNYRWhgdeBa"
      },
      "source": [
        "## 모듈 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U994LY6DdzLE",
        "outputId": "777386aa-9f6e-4476-c047-1e9e634d15a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandasql in /Users/chai/miniconda3/envs/nongnet/lib/python3.10/site-packages (0.7.3)\n",
            "Requirement already satisfied: numpy in /Users/chai/miniconda3/envs/nongnet/lib/python3.10/site-packages (from pandasql) (1.23.2)\n",
            "Requirement already satisfied: pandas in /Users/chai/miniconda3/envs/nongnet/lib/python3.10/site-packages (from pandasql) (1.4.3)\n",
            "Requirement already satisfied: sqlalchemy in /Users/chai/miniconda3/envs/nongnet/lib/python3.10/site-packages (from pandasql) (1.4.40)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/chai/miniconda3/envs/nongnet/lib/python3.10/site-packages (from pandas->pandasql) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/chai/miniconda3/envs/nongnet/lib/python3.10/site-packages (from pandas->pandasql) (2022.2.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /Users/chai/miniconda3/envs/nongnet/lib/python3.10/site-packages (from sqlalchemy->pandasql) (1.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/chai/miniconda3/envs/nongnet/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->pandasql) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandasql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pYTIBn6Kdaks"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from glob2 import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandasql import sqldf\n",
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "# 경고 끄기\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "# 시드고정\n",
        "tf.random.set_seed(19970119)\n",
        "random.seed(19970119)\n",
        "np.random.seed(19970119)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1tQSv1cd57m"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvxJh0SDd5WA"
      },
      "source": [
        "## 전처리 Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "m7u5Cj_On8JL"
      },
      "outputs": [],
      "source": [
        "class preprocessing_data(object):\n",
        "\n",
        "    \"\"\"\n",
        "    도매, 소매, 수입수출, 도매경락, 주산지 데이터 전처리용 class\n",
        "    중간결과물 저장 check parameter을 통해 지정, 중간결과물 저장 없이 사용은 check = 0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,dir):\n",
        "        \"\"\"\n",
        "        전체 데이터에서 해당하는 domae,imexport,pummok,somae,weather 별 분리\n",
        "        \"\"\"\n",
        "        self.data_list = glob(dir)\n",
        "        self.domae = []\n",
        "        self.imexport = []\n",
        "        self.pummok = []\n",
        "        self.somae = []\n",
        "        self.weather = []\n",
        "\n",
        "\n",
        "        for i in self.data_list:\n",
        "            if 'domae' in i:\n",
        "                self.domae.append(i)\n",
        "            if 'imexport' in i:\n",
        "                self.imexport.append(i)\n",
        "            if 'pummok' in i.split('/')[-1]:\n",
        "                self.pummok.append(i)\n",
        "            if 'somae' in i:\n",
        "                self.somae.append(i)\n",
        "            if 'weather' in i:\n",
        "                self.weather.append(i)\n",
        "\n",
        "\n",
        "    def add_pummock(self,check=0):\n",
        "\n",
        "        \"\"\"\n",
        "        check = 중간 산출물을 저장하고 싶다면 check 을 0 이외의 숫자로\n",
        "        pummock의 데이터를 가져와 '해당일자_전체거래물량', '하위가격 평균가', '상위가격 평균가', '하위가격 거래물량', '상위가격 거래물량' 의 파생변수를 생성하는 단계\n",
        "        \"\"\"\n",
        "\n",
        "        for num in tqdm(self.pummok):\n",
        "            ddf = pd.read_csv(num)  # pummock의 csv 읽어오기\n",
        "            name = num.split('/')[-1] # 전체 정제한 데이터를 담을 변수 이름\n",
        "\n",
        "            sep2 = sqldf(f\"select *, sum(거래량) as '해당일자_전체거래물량(kg)' from ddf group by datadate\")\n",
        "            # sql 문법을 이용해 '해당일자_전체거래물량' 계산\n",
        "\n",
        "            height_set = []\n",
        "            low_set = []\n",
        "            height_volume_set = []\n",
        "            low_volume_set = []\n",
        "\n",
        "            for i in sep2['datadate']:\n",
        "\n",
        "                \"\"\"\n",
        "                sep2는 group by를 통해 각 일자가 합쳐진 상태 예를 들어 '201703' 이 5개 이렇게 있을때 sep2는 group 시켜서 '해당일자_전체거래물량'을 계산\n",
        "                이후 sep2 기준 20170101 and 20220630 사이의 날짜들에 해당하는 각 '201703' 마다 '해당일자_전체평균가격' 보다 큰지 아니면 작은지 판단\n",
        "                위 과정을 통해 '하위가격 평균가', '상위가격 평균가', '하위가격 거래물량', '상위가격 거래물량' 변수 생성\n",
        "                \"\"\"\n",
        "\n",
        "                new_list = ddf.loc[[d for d, x in enumerate(ddf['datadate']) if x == i]]\n",
        "                set_price = sep2.loc[list(sep2['datadate']).index(i)]['해당일자_전체평균가격(원)']\n",
        "\n",
        "                sum_he_as = sum(new_list['거래대금(원)'].iloc[n] for n, z in enumerate(new_list['단가(원)']) if z >= set_price)\n",
        "                sum_he_vo = sum(new_list['거래량'].iloc[n] for n, z in enumerate(new_list['단가(원)']) if z >= set_price)\n",
        "\n",
        "                sum_lo_as = sum(new_list['거래대금(원)'].iloc[n] for n, z in enumerate(new_list['단가(원)']) if z < set_price)\n",
        "                sum_lo_vo = sum(new_list['거래량'].iloc[n] for n, z in enumerate(new_list['단가(원)']) if z < set_price)\n",
        "\n",
        "                if sum_lo_vo != 0:\n",
        "                    low_set.append(sum_lo_as / sum_lo_vo)\n",
        "                    low_volume_set.append(sum_lo_vo)\n",
        "                else:\n",
        "                    low_set.append(np.nan)\n",
        "                    low_volume_set.append(np.nan)\n",
        "\n",
        "                if sum_he_vo != 0:\n",
        "                    height_set.append(sum_he_as / sum_he_vo)\n",
        "                    height_volume_set.append(sum_he_vo)\n",
        "                else:\n",
        "                    height_set.append(np.nan)\n",
        "                    height_volume_set.append(np.nan)\n",
        "\n",
        "            sep2['하위가격 평균가(원)'] = low_set\n",
        "            sep2['상위가격 평균가(원)'] = height_set\n",
        "\n",
        "            sep2['하위가격 거래물량(kg)'] = low_volume_set\n",
        "            sep2['상위가격 거래물량(kg)'] = height_volume_set\n",
        "\n",
        "\n",
        "            globals()[f'df_{name.split(\"_\")[1].split(\".\")[0]}'] = sep2.copy()\n",
        "\n",
        "\n",
        "            # 중간 산출물 저장\n",
        "            if check != 0:\n",
        "                if os.path.exists(f'./data') == False:\n",
        "                    os.mkdir(f'./data')\n",
        "\n",
        "                if os.path.exists(f'./data/품목') == False:\n",
        "                    os.mkdir(f'./data/품목')\n",
        "\n",
        "                sep2.to_csv(f'./data/품목/{name}', index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def add_dosomae(self, option=1, check=0):\n",
        "\n",
        "        \"\"\"\n",
        "        check = 중간 산출물을 저장하고 싶다면 check 을 0 이외의 숫자로\n",
        "        domae, somae 데이터를 가져와서 정제하는 단계\n",
        "        option parameter을 통한 도매, 소매 선택\n",
        "        \"\"\"\n",
        "\n",
        "        if option == 1:\n",
        "            df = self.domae\n",
        "            text = '도매'\n",
        "        else:\n",
        "            df = self.somae\n",
        "            text = '소매'\n",
        "\n",
        "        for i in tqdm(df):\n",
        "            test = pd.read_csv(i)\n",
        "            name = i.split('/')[-1]\n",
        "\n",
        "            sep = test.loc[(test['등급명'] == '상품') | (test['등급명'] == 'S과')]  # 모든 상품에 대해서 수행하지 않고 GRAD_NM이 '상품', 'S과' 만 해당하는 품목 가져옴\n",
        "            sep = sep[['datadate', '등급명', '조사단위(kg)', '가격(원)']]\n",
        "\n",
        "            sep.rename(columns={\"가격(원)\": \"가격\"}, inplace=True)\n",
        "\n",
        "            sep2 = sqldf(\n",
        "                f\"select datadate, max(가격) as '일자별_{text}가격_최대(원)', avg(가격) as '일자별_{text}가격_평균(원)', min(가격) as '일자별_{text}가격_최소(원)' from sep group by datadate\")\n",
        "\n",
        "            globals()[f'df_{name.split(\"_\")[1].split(\".\")[0]}'] = globals()[f'df_{name.split(\"_\")[1].split(\".\")[0]}'].merge(sep2, how='left')\n",
        "\n",
        "            # 중간 산출물 저장\n",
        "            if check != 0:\n",
        "                if os.path.exists(f'./data') == False:\n",
        "                    os.mkdir(f'./data')\n",
        "\n",
        "                if os.path.exists(f'./data/{text}') == False:\n",
        "                    os.mkdir(f'./data/{text}')\n",
        "\n",
        "                sep2.to_csv(f'./data/{text}/{name}', index=False)\n",
        "\n",
        "    def add_imexport(self,check=0):\n",
        "        \"\"\"\n",
        "        check = 중간 산출물을 저장하고 싶다면 check 을 0 이외의 숫자로\n",
        "        imexport 데이터 관련 정제, imexport 데이터는 월별 수입수출 데이터임으로 해당 월에 같은 값을 넣어주고 없는 것에는 np.nan\n",
        "        해당 품목에 대한 imexport 데이터가 없는 경우 np.nan으로 대체, 모든 품목의 데이터가 동일한 컬럼수를 가지기 위해 수행\n",
        "        \"\"\"\n",
        "\n",
        "        imex_cd = [i.split('_')[-1].split('.')[0] for i in self.imexport]\n",
        "\n",
        "        for i in tqdm(range(len(self.pummok))):\n",
        "\n",
        "            cd_number = self.pummok[i].split('_')[-1].split('.')[0]\n",
        "            file_name = 'imexport_' + self.pummok[i].split('pummok_')[1]\n",
        "\n",
        "\n",
        "            if cd_number in imex_cd:\n",
        "                test4 = pd.read_csv(self.imexport[imex_cd.index(cd_number)])\n",
        "\n",
        "                new_exim1 = []\n",
        "                new_exim2 = []\n",
        "                new_exim3 = []\n",
        "                new_exim4 = []\n",
        "                new_exim5 = []\n",
        "\n",
        "                for j in globals()[f'df_{cd_number}']['datadate']:\n",
        "                    target = j//100\n",
        "\n",
        "                    try:\n",
        "                        number = list(test4['datadate']).index(target)\n",
        "                        new_exim1.append(test4['수출중량(kg)'].iloc[number])\n",
        "                        new_exim2.append(test4['수출금액(달러)'].iloc[number])\n",
        "                        new_exim3.append(test4['수입중량(kg)'].iloc[number])\n",
        "                        new_exim4.append(test4['수입금액(달러)'].iloc[number])\n",
        "                        new_exim5.append(test4['무역수지(달러)'].iloc[number])\n",
        "                        \n",
        "                    except:\n",
        "                        new_exim1.append(np.nan)\n",
        "                        new_exim2.append(np.nan)\n",
        "                        new_exim3.append(np.nan)\n",
        "                        new_exim4.append(np.nan)\n",
        "                        new_exim5.append(np.nan)\n",
        "\n",
        "                df2 = pd.DataFrame()\n",
        "                df2['수출중량(kg)'] = new_exim1\n",
        "                df2['수출금액(달러)'] = new_exim2\n",
        "                df2['수입중량(kg)'] = new_exim3\n",
        "                df2['수입금액(달러)'] = new_exim4\n",
        "                df2['무역수지(달러)'] = new_exim5\n",
        "\n",
        "                globals()[f'df_{cd_number}'] = pd.concat([globals()[f'df_{cd_number}'], df2],axis=1)\n",
        "\n",
        "            else:\n",
        "                df2 = pd.DataFrame()\n",
        "                df2['수출중량(kg)'] = np.nan\n",
        "                df2['수출금액(달러)'] = np.nan\n",
        "                df2['수입중량(kg)'] = np.nan\n",
        "                df2['수입금액(달러)'] = np.nan\n",
        "                df2['무역수지(달러)'] = np.nan\n",
        "\n",
        "                globals()[f'df_{cd_number}'] = pd.concat([globals()[f'df_{cd_number}'], df2], axis=1)\n",
        "\n",
        "\n",
        "            if check != 0:\n",
        "                if os.path.exists(f'./data') == False:\n",
        "                    os.mkdir(f'./data')\n",
        "\n",
        "                if os.path.exists(f'./data/수출입') == False:\n",
        "                    os.mkdir(f'./data/수출입')\n",
        "\n",
        "                df2.to_csv(f'./data/수출입/{file_name}', index=False)\n",
        "\n",
        "    def add_weather(self, check=0):\n",
        "\n",
        "        \"\"\"\n",
        "        check = 중간 산출물을 저장하고 싶다면 check 을 0 이외의 숫자로\n",
        "        weather 품목별 주산지 데이터를 가져와 합치는 함수, 일부 품목의 주산지가 3개가 아닌 것에 대해서는 np.nan 값으로 합쳐줌\n",
        "        \"\"\"\n",
        "\n",
        "        for i in tqdm(self.pummok):\n",
        "            name = i.split('_')[-1].split('.')[0]\n",
        "            check_file = [j for j in self.weather if j.split('_')[-2] == name]\n",
        "\n",
        "\n",
        "            df = pd.DataFrame()\n",
        "            for d, j in enumerate(check_file):\n",
        "                weather_df = pd.read_csv(j)\n",
        "                new_exim1, new_exim2, new_exim3, new_exim4, new_exim5, new_exim6 = [], [], [], [], [], []\n",
        "\n",
        "\n",
        "                for k in globals()[f'df_{name}']['datadate']:\n",
        "                    try:\n",
        "                        number = list(weather_df['datadate']).index(k)\n",
        "\n",
        "                        new_exim1.append(weather_df['초기온도(℃)'].iloc[number])\n",
        "                        new_exim2.append(weather_df['최대온도(℃)'].iloc[number])\n",
        "                        new_exim3.append(weather_df['최저온도(℃)'].iloc[number])\n",
        "                        new_exim4.append(weather_df['평균온도(℃)'].iloc[number])\n",
        "                        new_exim5.append(weather_df['강수량(ml)'].iloc[number])\n",
        "                        new_exim6.append(weather_df['습도(%)'].iloc[number])\n",
        "                    except:\n",
        "                        new_exim1.append(np.nan)\n",
        "                        new_exim2.append(np.nan)\n",
        "                        new_exim3.append(np.nan)\n",
        "                        new_exim4.append(np.nan)\n",
        "                        new_exim5.append(np.nan)\n",
        "                        new_exim6.append(np.nan)\n",
        "\n",
        "\n",
        "                df[f'주산지_{d}_초기온도(℃)'] = new_exim1\n",
        "                df[f'주산지_{d}_최대온도(℃)'] = new_exim2\n",
        "                df[f'주산지_{d}_최저온도(℃)'] = new_exim3\n",
        "                df[f'주산지_{d}_평균온도(℃)'] = new_exim4\n",
        "                df[f'주산지_{d}_강수량(ml)'] = new_exim5\n",
        "                df[f'주산지_{d}_습도(%)'] = new_exim6\n",
        "\n",
        "            if len(check_file) < 3:\n",
        "                df[f'주산지_2_초기온도(℃)'] = np.nan\n",
        "                df[f'주산지_2_최대온도(℃)'] = np.nan\n",
        "                df[f'주산지_2_최저온도(℃)'] = np.nan\n",
        "                df[f'주산지_2_평균온도(℃)'] = np.nan\n",
        "                df[f'주산지_2_강수량(ml)'] = np.nan\n",
        "                df[f'주산지_2_습도(%)'] = np.nan\n",
        "\n",
        "            globals()[f'df_{name}'] = pd.concat([globals()[f'df_{name}'], df], axis=1)\n",
        "\n",
        "            if check !=0:\n",
        "                if os.path.exists(f'./data') == False:\n",
        "                    os.mkdir(f'./data')\n",
        "\n",
        "                if os.path.exists(f'./data/주산지') == False:\n",
        "                    os.mkdir(f'./data/주산지')\n",
        "\n",
        "                df.to_csv(f'./data/주산지/weather_{name}.csv', index=False)\n",
        "\n",
        "    def add_categorical(self, out_dir, data_type=\"train\", check=0):\n",
        "\n",
        "        \"\"\"\n",
        "        check = 중간 산출물을 저장하고 싶다면 check 을 0 이외의 숫자로\n",
        "        일자별 정보를 넣어주는 함수, 월별, 상순, 하순, 중순 을 원핫 인코딩을 통해 데이터로 넣어주는 함수\n",
        "        모델이 각 행마다의 정보에서 몇월인지 상순인지 하순인지 파악하며 훈련시키기 위한 변수\n",
        "        \"\"\"\n",
        "\n",
        "        for i in tqdm(self.pummok):\n",
        "            name = i.split('_')[-1].split('.')[0]\n",
        "\n",
        "            day_set = []\n",
        "            month_set = []\n",
        "\n",
        "            for k in globals()[f'df_{name}']['datadate']:\n",
        "                day = k % 100\n",
        "                month = k % 10000 // 100\n",
        "\n",
        "                if day <= 10:\n",
        "                    day_set.append('초순')\n",
        "                elif (day > 10) and (day <= 20):\n",
        "                    day_set.append('중순')\n",
        "                else:\n",
        "                    day_set.append('하순')\n",
        "\n",
        "                month_set.append(f'{month}월')\n",
        "\n",
        "            globals()[f'df_{name}']['일자구분'] = day_set\n",
        "            globals()[f'df_{name}']['월구분'] = month_set\n",
        "\n",
        "            globals()[f'df_{name}'] = pd.get_dummies(globals()[f'df_{name}'], columns=['일자구분', '월구분'])\n",
        "\n",
        "            if check !=0:\n",
        "                if os.path.exists(f'./data') == False:\n",
        "                    os.mkdir(f'./data')\n",
        "\n",
        "                if data_type != \"train\":\n",
        "                    if os.path.exists(f'./data/{data_type}') == False:\n",
        "                        os.mkdir(f\"./data/{data_type}\")\n",
        "                    if os.path.exists(f'./data/{data_type}/{out_dir}') == False:\n",
        "                        os.mkdir(f'./data/{data_type}/{out_dir}')\n",
        "                    globals()[f'df_{name}'].to_csv(f'./data/{data_type}/{out_dir}/{data_type}_{name}.csv', index=False)\n",
        "                else:\n",
        "                    if os.path.exists(f'./data/{out_dir}') == False:\n",
        "                        os.mkdir(f'./data/{out_dir}')\n",
        "                    globals()[f'df_{name}'].to_csv(f'./data/{out_dir}/{data_type}_{name}.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfvySP5EeHJY"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up7q7ZaeeP-d"
      },
      "source": [
        "## 데이터 다운 및 압축풀기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sen3zfxoeojm",
        "outputId": "1ace294c-6ce4-4d91-b4ac-cef5a46afcd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: data: File exists\n",
            "/Users/chai/myRepo/Competitions/nongnet/data\n"
          ]
        }
      ],
      "source": [
        "%mkdir data\n",
        "%cd data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI8owYB3euEv",
        "outputId": "1391e788-b274-4920-f9eb-62210cd3538d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: gdown\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1Vcm2K6XwNyPgZUE-UIlq5rg5kdDtVG_q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEpIOfh5fS9Z",
        "outputId": "f780312a-dce4-45bd-d14f-4c9e7a966be5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: gdown\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1I1dpHtemwzisu9P4ZB-TWUL7-w72L3iO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JRgdL4sU8Sf"
      },
      "source": [
        "- 정답 제출양식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpNSlCguU1xV",
        "outputId": "0f9e2bcc-c75a-4d01-cd23-97b91b929695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: gdown\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1bXFp41evVZ98CpPPI2f9pLdmqo9Wixef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tpO0O2LlfZPs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open /content/data/aT_test_raw.zip, /content/data/aT_test_raw.zip.zip or /content/data/aT_test_raw.zip.ZIP.\n",
            "unzip:  cannot find or open /content/data/aT_train_raw.zip, /content/data/aT_train_raw.zip.zip or /content/data/aT_train_raw.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip -qq  \"/content/data/aT_test_raw.zip\"\n",
        "!unzip -qq  \"/content/data/aT_train_raw.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sim4EqdleOyg"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wPpioa1eMX2"
      },
      "source": [
        "## 훈련 데이터 전처리 및 저장 (중간저장 X, 최종저장 O) - train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "wnvCAIT4WxDQ",
        "outputId": "f6abd408-557e-4add-b380-739ad2e8a5bf"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'glob' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./aT_train_raw/*.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m data\u001b[38;5;241m.\u001b[39madd_pummock()\n\u001b[1;32m      3\u001b[0m data\u001b[38;5;241m.\u001b[39madd_dosomae()\n",
            "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mpreprocessing_data.__init__\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28mdir\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    전체 데이터에서 해당하는 domae,imexport,pummok,somae,weather 별 분리\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_list \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m(\u001b[38;5;28mdir\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomae \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimexport \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
          ]
        }
      ],
      "source": [
        "data = preprocessing_data('./aT_train_raw/*.csv')\n",
        "data.add_pummock()\n",
        "data.add_dosomae()\n",
        "data.add_dosomae(option=2)\n",
        "data.add_imexport()\n",
        "data.add_weather()\n",
        "data.add_categorical('train', data_type=\"train\" ,check=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co8-kOjniPxo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh9wDIksvErH"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhBfk96yuxPH"
      },
      "source": [
        "## 검증 데이터셋 전처리 및 저장 (중간저장 X, 최종저장 O) - test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZtIuAOFh2x2"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    data = preprocessing_data(f'./aT_test_raw/sep_{i}/*.csv')\n",
        "    data.add_pummock()\n",
        "    data.add_dosomae()\n",
        "    data.add_dosomae(option=2)\n",
        "    data.add_imexport()\n",
        "    data.add_weather()\n",
        "    data.add_categorical(f'set_{i}', data_type=\"test\", check=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWsIlzqevYwJ"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-OTLfDXvbvX"
      },
      "source": [
        "## 입력 shape 및 형태 정의 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lmKfDzfu9Tp"
      },
      "outputs": [],
      "source": [
        "def make_Tensor(array):\n",
        "    return tf.convert_to_tensor(array, dtype=tf.float32)\n",
        "\n",
        "def astype_data(data):\n",
        "    df = data.astype(np.float32)\n",
        "    return make_Tensor(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0UYIIbavf3F"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzSS2gBcvg-A"
      },
      "source": [
        "## Transformer 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_hTmh7IvljV"
      },
      "source": [
        "- encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsjSitfdvfNa"
      },
      "outputs": [],
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8UiEifWvoO4"
      },
      "source": [
        "- build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZRwGsc3vnwI"
      },
      "outputs": [],
      "source": [
        "def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(28)(x) # 4주 예측\n",
        "    return keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OEE7TQsvvrW"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPEtzTrJvy84"
      },
      "source": [
        "## keras eraly stop, chekpoint 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tahe2wY8vr83"
      },
      "outputs": [],
      "source": [
        "def call_back_set(name, epoch, batch_size):\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
        "\n",
        "    if os.path.exists(f'./check') == False:\n",
        "        os.mkdir(f'./check')\n",
        "\n",
        "    filename = f'./check/{name}-{epoch}-{batch_size}.h5'\n",
        "\n",
        "    checkpoint = ModelCheckpoint(filename,\n",
        "                                 monitor='val_loss',\n",
        "                                 verbose=1,\n",
        "                                 save_best_only=True,\n",
        "                                 save_weights_only=True,\n",
        "                                 mode='auto'\n",
        "                                 )\n",
        "    return [early_stopping, checkpoint]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiH83x8Ov4Hy"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE5xZZwbv5PP"
      },
      "source": [
        "## Model 훈련 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JQWwQQrv3qc"
      },
      "outputs": [],
      "source": [
        "def train(x_train, y_train, x_val, y_val, name, epoch, batch_size, learning_rate = 0.001, verbose = 1):\n",
        "    model = build_model(\n",
        "    x_train.shape[1:],\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"mean_squared_error\",\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    )\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    with tf.device('/device:GPU:1'):\n",
        "        history1 = model.fit(\n",
        "            x_train, y_train,\n",
        "            epochs = epoch,\n",
        "            steps_per_epoch=len(x_train) / batch_size,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(x_val, y_val),\n",
        "            validation_steps=len(x_val) / batch_size,\n",
        "            shuffle=False,\n",
        "            callbacks=call_back_set(name, epoch, batch_size),\n",
        "            verbose=verbose)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8ZoGpxWwAWM"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BemGVdjwBw5"
      },
      "source": [
        "## 시점 윈도우 생성 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDD_c3buv99w"
      },
      "outputs": [],
      "source": [
        "def time_window(df, t, t_sep):\n",
        "    seq_len = t\n",
        "    seqence_length = seq_len + t_sep\n",
        "\n",
        "    result = []\n",
        "    for index in tqdm(range(len(df) - seqence_length)):\n",
        "        result.append(df[index: index + seqence_length].values)\n",
        "\n",
        "    return np.array(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmM1U849wEab"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8rP8wMiwMm7"
      },
      "source": [
        "## 데이터 불러오기 및 parameter 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPx288BJwDmv"
      },
      "outputs": [],
      "source": [
        "data_list = glob('./data//train/*.csv')\n",
        "epoch = 1000\n",
        "batch = 15\n",
        "tr_del_list = ['단가(원)', '거래량', '거래대금(원)', '경매건수', '도매시장코드', '도매법인코드', '산지코드 '] # train 에서 사용하지 않는 열\n",
        "ts_del_list = ['단가(원)', '거래량', '거래대금(원)', '경매건수', '도매시장코드', '도매법인코드', '산지코드 ', '해당일자_전체평균가격(원)'] # test 에서 사용하지 않는 열\n",
        "check_col = ['일자구분_중순', '일자구분_초순', '일자구분_하순','월구분_10월', '월구분_11월', '월구분_12월', '월구분_1월', '월구분_2월', '월구분_3월', \n",
        "             '월구분_4월','월구분_5월', '월구분_6월', '월구분_7월', '월구분_8월', '월구분_9월'] # 열 개수 맞추기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8ayr_sBwXf5"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV90ogq-wZlC"
      },
      "source": [
        "## Train 과정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIMdTeflwQsJ"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(data_list):\n",
        "    df_number = i.split(\"_\")[-1].split(\".\")[0]\n",
        "    df = pd.read_csv(i)\n",
        "\n",
        "    for j in df.columns:\n",
        "        df[j] = df[j].replace({' ': np.nan})\n",
        "\n",
        "    # 사용할 열 선택 및 index 설정\n",
        "    df.drop(tr_del_list, axis=1, inplace=True)\n",
        "    df.set_index('datadate', drop=True, inplace=True)\n",
        "\n",
        "    # nan 처리\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    # 변수와 타겟 분리\n",
        "    x, y = df[[i for i in df.columns if i != '해당일자_전체평균가격(원)']], df['해당일자_전체평균가격(원)']\n",
        "\n",
        "    # 2주 입력을 통한 이후 4주 예측을 위해 y의 첫 14일을 제외\n",
        "    y = y[14:]\n",
        "\n",
        "    # time series window 생성\n",
        "    data_x = time_window(x, 13, 1)\n",
        "    data_y = time_window(y, 27, 1)\n",
        "\n",
        "    # y의 길이와 같은 길이로 설정\n",
        "    xdata = data_x[:len(data_y)]\n",
        "    ydata = data_y\n",
        "\n",
        "    # train, validation 분리 (8 : 2)\n",
        "    x_train, x_val, y_train, y_val = train_test_split(xdata, ydata, test_size=0.2, shuffle=False, random_state=119)\n",
        "\n",
        "    # transformer 모델 훈련\n",
        "    transformer = train(astype_data(x_train), y_train, astype_data(x_val), y_val, f'transformer-{df_number}', epoch,\n",
        "                        batch)\n",
        "    transformer.load_weights(f'./check/transformer-{df_number}-{epoch}-{batch}.h5')\n",
        "\n",
        "    if os.path.exists(f'./model') == False:\n",
        "        os.mkdir(f'./model')\n",
        "\n",
        "    # 모델 저장\n",
        "    transformer.save(f'./model/transformer-{df_number}-{epoch}-{batch}.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuUtQSv6wm7N"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXzEPY6_woE6"
      },
      "source": [
        "## Test 과정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zec8tU0ywlG0"
      },
      "outputs": [],
      "source": [
        "zero_csv = [0 for i in range(14)]  # 시점이 비어있는 데이터 0으로 채우기 위한 변수\n",
        "\n",
        "for i in tqdm(range(10)):\n",
        "    data_list = glob(f'./data/test/set_{i}/*.csv')\n",
        "\n",
        "    for idx,j in enumerate(data_list):\n",
        "        df = pd.read_csv(j)\n",
        "\n",
        "        if len(df) == 0:\n",
        "            df['zero_non'] = zero_csv\n",
        "            df = df.fillna(0)\n",
        "            df.drop('zero_non', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "        file_number = j.split('test_')[1].split('.')[0]\n",
        "\n",
        "        # 사용할 열 선택, index 설정\n",
        "        df.drop(ts_del_list, axis=1, inplace=True)\n",
        "        df.set_index('datadate', drop=True, inplace=True)\n",
        "\n",
        "        # train input 과 형상 맞추기\n",
        "        add_col = [i for i in check_col if i not in df.columns]\n",
        "\n",
        "        for a in add_col:\n",
        "            df[a] = 0\n",
        "\n",
        "        # ' ' -> nan 으로 변경\n",
        "        for a in df.columns:\n",
        "            df[a] = df[a].replace({' ': np.nan})\n",
        "\n",
        "        # nan 처리\n",
        "        df = df.fillna(0)\n",
        "\n",
        "        # x_test  생성\n",
        "        df_test = astype_data(df.values.reshape(1, df.values.shape[0], df.values.shape[1]))\n",
        "\n",
        "\n",
        "        # model test\n",
        "        if os.path.exists('./model_output') == False:\n",
        "            os.mkdir('./model_output')\n",
        "\n",
        "        if os.path.exists(f'./model_output/set_{i}') == False:\n",
        "            os.mkdir(f'./model_output/set_{i}')\n",
        "\n",
        "        # 해당하는 모델 불러오기\n",
        "        model_test = tf.keras.models.load_model(f'./model/transformer-{file_number}-{epoch}-{batch}.h5')\n",
        "        pred = model_test.predict(df_test)\n",
        "\n",
        "\n",
        "        # 결과 저장\n",
        "        save_df = pd.DataFrame(pred).T\n",
        "        save_df.to_csv(f'./model_output/set_{i}/predict_{file_number}.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaiVnMjZVGIQ"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBzRP7HJVHaP"
      },
      "source": [
        "## 정답 제출 파일생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LnvUoVirpYL"
      },
      "outputs": [],
      "source": [
        "for k in tqdm(range(10)):\n",
        "\n",
        "  globals()[f'set_df_{k}'] = pd.DataFrame()\n",
        "  answer_df_list = glob(f'./model_output/set_{k}/*.csv') # 예측한 결과 불러오기\n",
        "  pum_list = glob(f'./aT_test_raw/sep_{k}/*.csv') # 기존 test input 불러오기\n",
        "  pummok = [a for a in pum_list if 'pummok' in a.split('/')[-1]]\n",
        "\n",
        "  for i in answer_df_list:\n",
        "    df = pd.read_csv(i)\n",
        "    number = i.split('_')[-1].split('.')[0]\n",
        "\n",
        "    base_number = 0\n",
        "    for p in pummok:\n",
        "      if number == p.split('_')[-1].split('.')[0]:\n",
        "        pum_df = pd.read_csv(p)\n",
        "\n",
        "        if len(pum_df) != 0:\n",
        "           base_number = pum_df.iloc[len(pum_df)-1]['해당일자_전체평균가격(원)']  # 기존 각 sep 마다 test input의 마지막 target 값 가져오기 (변동률 계산을 위해)\n",
        "        else:\n",
        "          base_number = np.nan\n",
        "\n",
        "    globals()[f'set_df_{k}'][f'품목{number}']  = [base_number] + list(df[df.columns[-1]].values) # 각 품목당 순서를 t, t+1 ... t+28 로 변경\n",
        "\n",
        "  globals()[f'set_df_{k}'] = globals()[f'set_df_{k}'][[f'품목{col}' for col in range(37)]] # 열 순서를 품목0 ~ 품목36 으로 변경"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snOtIhAGaCZ"
      },
      "source": [
        "- 변동률 계산을 위한 t, t+1 ... t+28 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KtiJup_xl00"
      },
      "outputs": [],
      "source": [
        "set_df_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaTBCR5y3DA5"
      },
      "source": [
        "- 변동률 계산 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUAw88bXupw7"
      },
      "outputs": [],
      "source": [
        "date = [f'd+{i}' for i in range(1,15)] + ['d+22 ~ 28 평균']\n",
        "\n",
        "\n",
        "for k in range(10):\n",
        "  globals()[f'answer_df_{k}'] = pd.DataFrame()\n",
        "  for c in globals()[f'set_df_{k}'].columns:\n",
        "    base_d = globals()[f'set_df_{k}'][c][0] # 변동률 기준 t 값\n",
        "\n",
        "    ans_1_14 = []\n",
        "    for i in range(14):\n",
        "      ans_1_14.append((globals()[f'set_df_{k}'][c].iloc[i+1]- base_d)/base_d)  # t+1 ~ t+14 까지는 (t+n - t)/t 로 계산\n",
        "\n",
        "    ans_22_28 = (globals()[f'set_df_{k}'][c][22:29].mean() - base_d)/base_d # t+22 ~ t+28은 np.mean(t+22 ~ t+28) - t / t\n",
        "\n",
        "    globals()[f'answer_df_{k}'][f'{c} 변동률'] = ans_1_14 + [ans_22_28]\n",
        "  \n",
        "  globals()[f'answer_df_{k}']['Set'] = k # set 번호 설정\n",
        "  globals()[f'answer_df_{k}']['일자'] = date # 일자 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoXfNkur3Hl4"
      },
      "source": [
        "- sep 0  ~ sep 9 까지 합치기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrEcol7zz0xY"
      },
      "outputs": [],
      "source": [
        "# 위에서 계산된 변동률 들을 합쳐주는 과정\n",
        "\n",
        "all_df =pd.DataFrame()\n",
        "for i in range(10):\n",
        "  if i== 0 :\n",
        "    all_df = pd.concat([all_df, globals()[f'answer_df_{i}']],axis=1)\n",
        "  else:\n",
        "    all_df = pd.concat([all_df, globals()[f'answer_df_{i}']])\n",
        "\n",
        "\n",
        "all_df = all_df[['Set','일자'] + list(all_df.columns[:-2])]\n",
        "all_df.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiRvcBaMD82b"
      },
      "source": [
        "- 정답 양식으로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDCkhEIz10g1"
      },
      "outputs": [],
      "source": [
        "# set, 일자 기억하기위해 따로 저장\n",
        "\n",
        "re_set = list(all_df['Set'])\n",
        "re_date = list(all_df['일자'])\n",
        "\n",
        "\n",
        "# 정답 양식 불러오기\n",
        "out_ans = pd.read_csv('./answer_example.csv')\n",
        "\n",
        "# 두 dataframe 합치기 (nan + 숫자 = nan 이용)\n",
        "submit_df = all_df + out_ans\n",
        "\n",
        "submit_df['Set'] = re_set\n",
        "submit_df['일자'] = re_date\n",
        "\n",
        "\n",
        "# 최종 저장\n",
        "submit_df.to_csv('./submit.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDA0Hq3jFv39"
      },
      "source": [
        "- 계산된 변동률 결과물"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mKkxdrSFzpm"
      },
      "outputs": [],
      "source": [
        "all_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4ZIqhpdF26-"
      },
      "source": [
        "- 제출 양식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgBJdo0iF45J"
      },
      "outputs": [],
      "source": [
        "out_ans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQbCc_EJFuJL"
      },
      "source": [
        "- 제출 양식 반영한 최종 결과물 (**실 제출용**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd7vYCmaERDh"
      },
      "outputs": [],
      "source": [
        "submit_df"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "aT_베이스라인.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "193ef9687787e96da570fc28a54b0c74b970c272a6e2797c194cbcc742706286"
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit ('nongnet': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
